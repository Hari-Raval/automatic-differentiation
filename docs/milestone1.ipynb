{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Describe the problem the software solves and why it's important to solve that problem.}$\n",
    "\n",
    "* Our software package computes gradients by leveraging the technique of automatic differentiation. Before we can understand automatic differentiation, we must first describe and motivate the importance of differentiation itself. Finding the derivative of a function measures the sensitivity to change of a function value with respect to a change in its input argument. Derivatives are not only essential in calculus applications like numerically solving differential equations and optimizing and solving linear systems, but are useful in many real world, scientific settings. For example, differentiation is essential in analyzing the profit and loss of a company's business or finding the minimum amount of material to construct a building.   \n",
    "\n",
    "\n",
    "* To perform differentiation, two different approaches are solving the task symbolically or numerically computing the derivatives. Symbolic differentation yields accurate answers, however depending on the complexity of the function, it could be expensive to evaluate and result in inefficient code. On the other hand, numerically computing derivatives is less expensive, however it suffers from potential issues with stability and a loss of accuracy.   \n",
    "\n",
    "\n",
    "* Automatic differentiation overcomes the shortcomings of both the symbolic and numerical approach. Automatic differentiation is less costly than symbolic differentiation, but evaluates derivatives at machine precision. The technique leveages both forward mode and reverse mode and evaluates each step with the results of previous computations or values. As a result of this, autuomatic differentiation avoids finding an analytical expresssion for the derivative itself and is thus iteratively evaluating a gradient based on input values. Thus, based on these key advantages, our library implements and performs forward mode automatic differentiation to efficiently and accurately compute derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Describe (briefly) the mathematical background and concepts as you see fit.  \n",
    "You do not need to give a treatise on automatic differentiation or dual numbers. Just give the essential ideas (e.g. the chain rule, the graph structure of calculations, elementary functions, etc).}$\n",
    "\n",
    "\n",
    "#### 1: Chain Rule \n",
    "\n",
    "At the heart of automatic differentiation is the Chain Rule that enables us to decompose a complex derivative into a set of derivatives involving elementary functions of which we know explicit forms. \n",
    "\n",
    "We will first introduce the case of 1-D input and generalize it to multidimensional inputs.\n",
    "\n",
    "1-D input: Suppose we have a function $ h(u(t)) $ and we want to compute the derivative of $ h $ with respect to $ t $. This derivative is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dh}{dt} = \\frac{\\partial h}{\\partial u} \\frac{du}{dt}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Before introducing vector inputs, let's first take a look at the gradient operator $ \\nabla $\n",
    "\n",
    "That is, for  $ y\\colon \\mathbb {R} ^{n} \\to \\mathbb {R} $, its gradient $ \\nabla y \\colon \\mathbb {R} ^{n} \\to \\mathbb {R} ^{n}$ is defined at the point $ x = (x_1, ..., x_n) $ in n-dimensional space as the vector\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla y(x) =\n",
    "\\begin{bmatrix}\n",
    "{\\frac {\\partial y}{\\partial x_{1}}}(x)\n",
    "\\\\\n",
    "\\vdots \n",
    "\\\\\n",
    "{\\frac {\\partial y}{\\partial x_{n}}}(x)\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Multidimensional (or Vector) inputs: Suppose we have a function $ h(y_1(x), ..., y_n(x)) $ and we want to compute the derivative of $ h $ with respect to $ x $. This derivative is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla h_x = \\sum_{i=1}^n \\frac{\\partial h}{\\partial y_i} \\nabla y_i(x)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 2: Evaluation (Forward) Trace\n",
    "Definition: Suppose x = $ \\begin{bmatrix} {x_1} \\\\ \\vdots \\\\ {x_m} \\end{bmatrix} $, we defined $ v_{k - m} = x_k $ for $ k = 1, 2, ..., m $ in the evaluation trace.\n",
    "\n",
    "Motivation: The evaluation trace introduces intermediate results $ v_{k-m} $ of elementary unary or binary operations.\n",
    "\n",
    "Consider the function $ f(x):\\mathbb{R}^2 \\to \\mathbb{R} $\n",
    "\n",
    "$ f(x) = x_1 x_2 + sin(x_1 x_2) $\n",
    "\n",
    "We want to evaluate the gradient $ \\nabla f $ at the point $ x = \\begin{bmatrix} 7 \\\\ 4 \\end{bmatrix} $. Computing gradient by hand gives: \n",
    "\n",
    "$ \\nabla f = \\begin{bmatrix} \\frac {\\partial f} {\\partial x_1} \\\\ \\frac {\\partial f} {\\partial x_2} \\end{bmatrix}  = \\begin{bmatrix} x_2 + x_2 sin(x_1 x_2) \\\\ x_1 + x_1 sin(x_1 x_2) \\end{bmatrix} = \\begin{bmatrix} 4 + 4sin(28) \\\\ 7 + 7sin(28) \\end{bmatrix}$\n",
    "\n",
    "| Forward primal trace | Forward tangent trace | Pass with p = $[0, 1]^T$ | Pass with p = $[1, 0]^T$ |\n",
    "| --- | --- | --- | --- |\n",
    "| $v_{-1} = x_1$ | $ p_1 $ | 1 | 0 |\n",
    "| $v_{0} = x_2$ | $ p_2 $ | 0 | 1 |\n",
    "| $v_{1} = v_{-1} v_0$ | $ v_0 D_p v_{-1} + v_{-1} D_p v_0 $ | 4 | 7 |\n",
    "| $v_{2} = sin(v_{-1} v_0)$ | $ cos(v_{-1} v_0) D_p v_{1} $ | 4 cos(28) | 7 cos(28) |\n",
    "| $v_{3} = v_{1} + v_{2}$ | $ D_p v_1 + D_p v_2 $ | 4 + 4 cos(28) | 7 + 7 cos(28) |\n",
    "\n",
    "$D_p v_{-1} = \\nabla v_{-1}^T p = (\\frac {\\partial v_{-1}} {\\partial x_1} \\nabla x_{1})^T p = (\\nabla x_{1})^T p = p_1$\n",
    "\n",
    "$D_p v_{0} = \\nabla v_{0}^T p = (\\frac {\\partial v_{0}} {\\partial x_2} \\nabla x_{2})^T p = (\\nabla x_{2})^T p = p_2$\n",
    "\n",
    "$D_p v_{1} = \\nabla v_{1}^T p = (\\frac {\\partial v_{1}} {\\partial v_{-1}} \\nabla v_{-1} + \\frac {\\partial v_{1}}{\\partial v_{0}} \\nabla v_{0})^T p = (v_0 \\nabla v_{-1} + v_{-1} \\nabla v_0)^T p = v_0 D_p v_{-1} + v_{-1} D_p v_0$\n",
    "\n",
    "$D_p v_{2} = \\nabla v_{2}^T p = (\\frac {\\partial v_{2}} {\\partial v_{-1}} \\nabla v_{-1} + \\frac {\\partial v_{2}}{\\partial v_{0}} \\nabla v_{0})^T p = (v_0 cos(v_{-1} v_0) \\nabla v_{-1} + v_{-1} cos(v_{-1} v_0) \\nabla v_0)^T p = cos(v_{-1} v_0) (v_0 D_p v_{-1} + v_{-1} D_p v_0) = cos(v_{-1} v_0) D_p v_{1}$\n",
    "\n",
    "$D_p v_{3} = \\nabla v_{3}^T p = (\\frac {\\partial v_{3}} {\\partial v_{1}} \\nabla v_{1} + \\frac {\\partial v_{3}}{\\partial v_{2}} \\nabla v_{2})^T p = D_p v_1 + D_p v_2$\n",
    "\n",
    "#### 3: Computation (Forward) Graph\n",
    "\n",
    "We have associated each $ v_{k-m} $ to a node in a graph for a visualization of the partial ordering.\n",
    "\n",
    "From above example, its computational graph is given by: \n",
    "\n",
    "![title](./computation.png)\n",
    "\n",
    "#### 4: Computing the derivative\n",
    "\n",
    "Let's return to the gradient $ \\nabla $\n",
    "\n",
    "Definition of gradient operator: we project the gradient from before in the direction of $ p $\n",
    "\n",
    "$$ D_p v_j = (\\nabla v_j)^T p = (\\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} \\nabla v_i)^T p = \\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} (\\nabla v_i)^T p = \\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} D_p v_i$$ \n",
    "\n",
    "Higher dimension: We recursively apply the same technic introduced above to each entry of the vector valued function f\n",
    "\n",
    "Two take away messages: \n",
    "\n",
    "1) We can compute the derivative of $ v_j $ with knowledge of $ v_i $ and $ D_p v_i $ for $ i < j $.\n",
    "\n",
    "2) Once a child node is evaluated, its parent node(s) are no longer needed. There is no need to store the full graph of and pairs.\n",
    "\n",
    "#### 5: Dual Number\n",
    "\n",
    "Definition: we define a dual number $ z_j = v_j + D_p v_j \\epsilon $ such that $ \\epsilon^2 = 0 $ where $ v_j $ corresponds to primal trace and $ D_p v_j $ corresponds to tangent trace. \n",
    "\n",
    "$ f(z_j) = f(v_j + D_p v_j \\epsilon) = f(v_j) + f'(v_j) D_p v_j \\epsilon $ using a Taylor series expansion\n",
    "\n",
    "All higher term vanish because of the definition $ \\epsilon^2 = 0 $\n",
    "\n",
    "Advantage: Operations on Dual Number pertain the form of Taylor expansion.\n",
    "\n",
    "Consider the following example\n",
    "$$\n",
    "\\begin{align}\n",
    "z_1 &= a_1 + b_1 \\epsilon \\\\ \n",
    "z_2 &= a_2 + b_2 \\epsilon \\\\\n",
    "z_1 + z_2 &= (a_1 + a_2) + (b_1 + b_2) \\epsilon \\\\\n",
    "z_1 z_2 &= a_1 a_2 + (a_1 b_2 + a_2 b_1) \\epsilon \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We subsititute $ a_1 = u, b_1 = u' $ and $ a_2 = v $ and $ b_2 = v' $,\n",
    "$$\n",
    "\\begin{align}\n",
    "z_1 + z_2 &= (u + v) + (u' + v') \\epsilon \\\\\n",
    "z_1 z_2 &= u v + (u v' + u' v) \\epsilon \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use ad-AHJZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{How do you envision that a user will interact with your package? What should they import? How can they instantiate AD objects?}$\n",
    "\n",
    "1. __Installing the package:__\n",
    "    * 1a. User starts their virtual environment and installs our package with the following line\n",
    "    \n",
    "    <code>python3 -m pip install AHJZ_autodiff </code>\n",
    "      \n",
    "    * 1b. User installs dependencies from requirements.txt with the following command: \n",
    "\n",
    "    <code>python3 -m pip install requirements.txt </code>\n",
    "\n",
    "\n",
    "2. __Importing the package:__\n",
    "\n",
    "    * 2a. User imports package into their desired python environment with the following line\n",
    "    \n",
    "    ```python\n",
    "    from ad-AHJZ import functions, forward-mode, extension-module as ad\n",
    "    ```\n",
    "    \n",
    "    * 2b. User imports numpy into their desired python environment with the following line:  \n",
    "    \n",
    "    ```python\n",
    "    import numpy as np\n",
    "    ```\n",
    "    \n",
    "    \n",
    "3. __Calling/Using package modules:__\n",
    "    * 3a. User makes use of the prefix \"ad\" and can call the different methods listed in the following files to perform automatic differentiation on both scalar and vector quantities:  \n",
    "    \n",
    "    * 3b. Example Function calls for scalar case\n",
    "    \n",
    "    ```python\n",
    "    # define desired evaluation value (scalar)\n",
    "    temp = np.array([0.5]) \n",
    "    # define a function to fund the derivative of\n",
    "    f_x = math.sin(temp) + 2 * temp\n",
    "    # call forward mode method\n",
    "    fm = ad.forwardmode(evaluate = temp, function = f_x)\n",
    "    # store function value, first derivative, last operations done to obtain the function value, last operation done  to obtain the derivative value\n",
    "    x, x_der, x_expression, x_der_expression = fm\n",
    "    # extract and store the function value and first derivative\n",
    "    val = x\n",
    "    df_dx = x_der\n",
    "\n",
    "    ```\n",
    "    * 3c. Example Function calls for vector case\n",
    "    ```python    \n",
    "    # define desired evaluation value (scalar)\n",
    "    temp = np.array([0.5, 0.5, 1, 1, 2]) \n",
    "    # define a function to fund the derivative of\n",
    "    f_x = math.cos(temp) + 10 * temp\n",
    "    # call forward mode method\n",
    "    fm = ad.forwardmode(evaluate = temp, function = f_x)\n",
    "    # store function value, first derivative, last operations done to obtain the function value, last operation done  to obtain the derivative value\n",
    "    x, x_der, x_expression, x_der_expression = fm\n",
    "    # extract and store the function value and first derivative\n",
    "    val = x\n",
    "    df_dx = x_der\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Discuss how you plan on organizing your software package}$\n",
    "\n",
    "\n",
    "1. __Directory Structure:__\n",
    "    * 1a. We include our project directory structure in the image below. Our package is called AHJZ_autodiff, where our code for automatic differentiation lies within \"AHJZ_autodiff\", our milestone documentation lies within \"docs\", all unit testing files are located in \"testing\", and the root of the directory holds our readme, license, and requirements.txt file.\n",
    "    \n",
    "    * 1b. Directory structure layout: ![image info](./auto_diff_project_directory_structure.png)\n",
    "\n",
    "\n",
    "2. __Modules:__\n",
    "    * 2a. _functions.py_: This file contains sixteen methods to compute the function values and derivitives of the elementary operations outlined below. These functions form the partial computational pieces required to perform automatic differentiation. The sixteen elementary functions are the following: '+', '-', '*' '/', 'sqrt(x)', 'power(x,n)', 'exp(x)', 'log(x, b)', 'ln(x)', 'sin(x)', 'cos(x)', 'tan(x)', 'cot(x)', 'csc(x)', 'sec(x)'.\n",
    "    \n",
    "    * 2b. _forward_mode.py_: This file computes the gradient using the forward mode of automatic differentiation. A user is required to input the function they are interested in computing the derivative of and the point at which the derivative is to be evaluated at.\n",
    "    \n",
    "    * 2c. _fast_forward_mode.py_ (extension module): This file will contain our extension to the basic automatic differentiation functionality. In this module we compute the gradient using an efficient version of forward mode of automatic differentation by using an efficient graph data structure and optimized tree traversal. A user is required to input the function they are interested in computing the derivative of and the point at which the derivative is to be evaluated at. \n",
    "\n",
    "\n",
    "3. __Test Suite Location:__\n",
    "    * 3a. The test suite will live in the \"testing\" directory which is a subdirectory found off the root directory (see 1. Directory Structure). The \"testing\" directory will contain all unit tests for our different modules. \n",
    "    \n",
    "    * 3b. To ensure our testing procedure has complete code coverage, we will leverage CodeCov. CodeCov will enable us to quickly understand which lines are being executed in our test cases. Moreover, we will make use of TravisCI in order to see which of our unit tests are passing/failing. \n",
    "    \n",
    "    \n",
    "4. __Package Distribution:__\n",
    "    * 4a. The package will be distributed via PyPI. To deploy our package on PyPI and make it available for others to use, we would also need to add setup.py and a setup.cfg files. After creating an account, a user would execute the setup.py file and can distribute the project. \n",
    "\n",
    "\n",
    "5. __Packaging the Software:__\n",
    "    * 5a. The software will be packaged using...?\n",
    "\n",
    "\n",
    "6. __Other Considerations: Package Dependencies__\n",
    "    * 6a. The only library dependency our package will rely on is numpy. We design our software this way to ensure that we are not creating multiple external dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Discuss how you plan on implementing the forward mode of automatic differentiation.}$\n",
    "\n",
    "1. __Core Data Structure:__\n",
    "    * 1a. Our primary core data structure is going to be a dictionary to store each node in the computational graph that uses the tangent trace and primal trace to compute the partial differentiation and function value for a specific variable. More specifically, the keys would be the node of the computational graph (state name) and the values are going to be a tuple that holds the associated operation at the specific state (function value and derivative).  \n",
    "      \n",
    "      \n",
    "2. __Classes:__\n",
    "    \n",
    "    * 2a. _Dual Numbers_: Class which represents a dual number    \n",
    "    \n",
    "    * 2b. _Forward Mode_: Class which represents the computational graph and performs forward mode differentiation  \n",
    "     \n",
    "    * 2c. _Fast Forward Mode_ (extension module): Class which represents an optimized compututional graph and perfroms a faster version of forward mode differentiation by efficiently storing the graph and doing clever graph traversal.\n",
    "    \n",
    "    \n",
    "3. __Method and Name Attributes:__\n",
    "    * 3a. _Dual Numbers_:\n",
    "        * Method to initialize the real value and dual number value for the input of the function          \n",
    "        * Methods to overload the elementary operations for a varaible that is dual number\n",
    "            * For example, we would use the below method to overload the \"add\" operation: \n",
    "            ```python  \n",
    "               def __add__(self, other):\n",
    "                        if isinstance(other, DualNumber):\n",
    "                            return DualNumber(self.real + other.real, self.dual + other.dual)\n",
    "            ```\n",
    "    * 3b. _Forward Mode_:\n",
    "        * Method to initialize the computational graph dictionary, its initial _x_ value, and an empty dictionary of the primal trace and tangent trace to perform forward mode automatic differentiation\n",
    "        * Method to iterate through the input function and append to the primal and tangent trace dictionary\n",
    "        * Method to get the node value and partial derivative at the current step of forward mode\n",
    "        * Method to obtain the primal trace to arrive at the next node's value and partial derivative\n",
    "        * Method to obtain the tangent trace to arrive at the next node's value and partial derivative\n",
    "        * Method to run the entire forward mode process and obtain the final function value and derivative from the computational graph.  \n",
    "          \n",
    "    * 3c. _Fast Forward Mode_:\n",
    "        * Method to initialize the optimized computational graph data structure, its initial _x_ value, and an empty optimized structure of the primal and tangent trace to perform forward mode automatic differentiation\n",
    "        * Method to iterate through the input function and append to the primal and tangent trace data structure\n",
    "        * Method to get the node value and partial derivative at the current step of forward mode\n",
    "        * Method to obtain the primal trace to arrive at the next node's value and partial derivative\n",
    "        * Method to obtain the tangent trace to arrive at the next node's value and partial derivative\n",
    "        * Method to efficiently run the entire forward mode process and obtain the final function value and derivative from the optimized computational graph.\n",
    "    \n",
    "   \n",
    "4. __External Dependencies:__\n",
    "    * 4a. The only external library we will rely on is numpy, which we will use to perform computations and evaluate small expressions with. With this being our only external dependency, our software increases its reliability and can be viewed as a near stand alone software package. \n",
    "    \n",
    "    \n",
    "5. __Dealing With Elementary Functions__\n",
    "    * 5a. For all elementary functions like _sin_, _sqrt_, _log_, and _exp_ (and all the others mentioned in _Modules_) we will define separate methods for them in functions.py. This module will generalize each of the functions in order to handle both scalar and vector input. Each method will take in as input a vector or scalar value stored at the previous node in the computational graph and output the derivative value and function value for the elementary function. We can then store the methods' outputs as a tuple in the computational graph dictionary. \n",
    "    * 5b. For example, we would use the below functions to implement _sin_ and _sqrt_, both of which work with scalar or vector input _x_ values: \n",
    "```python  \n",
    "    def sin(x): \n",
    "        x_val = np.sin(x)\n",
    "        x_der = np.cos(x)\n",
    "        return (x_val, x_der)\n",
    "```\n",
    "```python  \n",
    "    def sqrt(x): \n",
    "        x_val = np.sqrt(x)\n",
    "        x_der = 0.5 * np.power(x, -0.5)\n",
    "        return (x_val, x_der)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Briefly motivate your license choice}$\n",
    "\n",
    "Our AHJZ_autodiff package is licensed under the GNU General Public License v3.0. This free software license allows users to do just about anything they want with our project, except distribute closed source versions. This means that any improved versions of our package that individuals seek to release must also be free software. We find it essential to allow users to help each other share their bug fixes and improvements with other users. Our hope is that users of this package continually find ways to improve it and share these improvements within the broader scientific community that uses automatic differentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
